{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "X93SodzAv9hV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robocarstore/donkey-car-training-on-google-colab/blob/master/Donkey_Car_Training_using_Google_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlmQIFSLZDdc"
      },
      "source": [
        "# Donkey Car Training using Google Colab\n",
        "\n",
        "Train your donkey car model using accelerated GPU for FREE on Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arsH-DhLcihq"
      },
      "source": [
        "## Check GPU and Tensorflow version\n",
        "If \"Found GPU at: / device: GPU: 0\" is displayed, the GPU is ready to use."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "ORYT2sEa9Vx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verified tensorflow version: > 2.9 and <= 2.15"
      ],
      "metadata": {
        "id": "Xf2CZgLm9gGk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQgEhuoTcg0N"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba2oPDIrsDFg"
      },
      "source": [
        "## Git Clone the donkey repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOxd9PFUyNxI",
        "outputId": "7afc89a3-288d-4c62-d466-d816a3ce7a94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/robocarstore/donkeycar.git\n",
        "%cd /content/donkeycar"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'donkeycar'...\n",
            "remote: Enumerating objects: 15444, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 15444 (delta 17), reused 22 (delta 11), pack-reused 15396\u001b[K\n",
            "Receiving objects: 100% (15444/15444), 99.88 MiB | 32.31 MiB/s, done.\n",
            "Resolving deltas: 100% (9907/9907), done.\n",
            "/content/donkeycar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout v5"
      ],
      "metadata": {
        "id": "u6nZMEOSmBy3",
        "outputId": "c782375e-921c-4664-a774-4ed54cfb97f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'v5' set up to track remote branch 'v5' from 'origin'.\n",
            "Switched to a new branch 'v5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TkkcF-gsAnx"
      },
      "source": [
        "## Install donkey car"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install --upgrade pip setuptools wheel\n",
        "!pip3 install -e .[pc]"
      ],
      "metadata": {
        "id": "bhfs7OmMlXYT",
        "outputId": "0719bc90-0d98-4574-c785-52f6198513e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/donkeycar\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from donkeycar==5.0.dev3) (1.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from donkeycar==5.0.dev3) (9.4.0)\n",
            "Collecting docopt (from donkeycar==5.0.dev3)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from donkeycar==5.0.dev3) (6.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from donkeycar==5.0.dev3) (2.31.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from donkeycar==5.0.dev3) (3.9.0)\n",
            "Requirement already satisfied: PrettyTable in /usr/local/lib/python3.10/dist-packages (from donkeycar==5.0.dev3) (3.9.0)\n",
            "Collecting paho-mqtt (from donkeycar==5.0.dev3)\n",
            "  Downloading paho_mqtt-2.0.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simple_pid (from donkeycar==5.0.dev3)\n",
            "  Downloading simple_pid-2.0.0-py3-none-any.whl (7.2 kB)\n",
            "Collecting progress (from donkeycar==5.0.dev3)\n",
            "  Downloading progress-1.6.tar.gz (7.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from donkeycar==5.0.dev3) (4.9.0)\n",
            "Collecting pyfiglet (from donkeycar==5.0.dev3)\n",
            "  Downloading pyfiglet-1.0.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from donkeycar==5.0.dev3) (5.9.5)\n",
            "Collecting pynmea2 (from donkeycar==5.0.dev3)\n",
            "  Downloading pynmea2-1.19.0-py3-none-any.whl (30 kB)\n",
            "Collecting pyserial (from donkeycar==5.0.dev3)\n",
            "  Downloading pyserial-3.5-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting utm (from donkeycar==5.0.dev3)\n",
            "  Downloading utm-0.7.0.tar.gz (8.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from donkeycar==5.0.dev3) (1.5.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from donkeycar==5.0.dev3) (6.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from donkeycar==5.0.dev3) (3.7.1)\n",
            "Collecting kivy (from donkeycar==5.0.dev3)\n",
            "  Downloading Kivy-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.2/21.2 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from donkeycar==5.0.dev3) (5.15.0)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (from donkeycar==5.0.dev3) (1.3.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations->donkeycar==5.0.dev3) (1.11.4)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations->donkeycar==5.0.dev3) (0.19.3)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations->donkeycar==5.0.dev3) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations->donkeycar==5.0.dev3) (4.9.0.80)\n",
            "Collecting Kivy-Garden>=0.1.4 (from kivy->donkeycar==5.0.dev3)\n",
            "  Downloading Kivy_Garden-0.1.5-py3-none-any.whl (4.6 kB)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.10/dist-packages (from kivy->donkeycar==5.0.dev3) (0.18.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from kivy->donkeycar==5.0.dev3) (2.16.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->donkeycar==5.0.dev3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->donkeycar==5.0.dev3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->donkeycar==5.0.dev3) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->donkeycar==5.0.dev3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->donkeycar==5.0.dev3) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->donkeycar==5.0.dev3) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->donkeycar==5.0.dev3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->donkeycar==5.0.dev3) (2023.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->donkeycar==5.0.dev3) (8.2.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from PrettyTable->donkeycar==5.0.dev3) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->donkeycar==5.0.dev3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->donkeycar==5.0.dev3) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->donkeycar==5.0.dev3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->donkeycar==5.0.dev3) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->donkeycar==5.0.dev3) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations->donkeycar==5.0.dev3) (1.2.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations->donkeycar==5.0.dev3) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations->donkeycar==5.0.dev3) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations->donkeycar==5.0.dev3) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations->donkeycar==5.0.dev3) (1.5.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->donkeycar==5.0.dev3) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->donkeycar==5.0.dev3) (3.2.0)\n",
            "Building wheels for collected packages: docopt, progress, utm\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=568819e4dcbb5bd49ac6625da616daff79e4698236a59658a2702b6fbc033f4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for progress (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progress: filename=progress-1.6-py3-none-any.whl size=9612 sha256=8c99f89e3bfb59894515df7983722b40b4725ad9c40e3ffbd066de06fbd3927f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/68/5f/c339b20a41659d856c93ccdce6a33095493eb82c3964aac5a1\n",
            "  Building wheel for utm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utm: filename=utm-0.7.0-py3-none-any.whl size=6084 sha256=395126ae6e896d4eb012e46425441c09b5245e8c8a7f22dc36ef6d4ddf785ba7\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/a1/c8/543df0e8f5e824c3e92a432e32deb9cd89ae686095ee8cfcbe\n",
            "Successfully built docopt progress utm\n",
            "Installing collected packages: utm, pyserial, pynmea2, progress, docopt, simple_pid, pyfiglet, paho-mqtt, Kivy-Garden, kivy, donkeycar\n",
            "  Running setup.py develop for donkeycar\n",
            "Successfully installed Kivy-Garden-0.1.5 docopt-0.6.2 donkeycar-5.0.dev3 kivy-2.3.0 paho-mqtt-2.0.0 progress-1.6 pyfiglet-1.0.2 pynmea2-1.19.0 pyserial-3.5 simple_pid-2.0.0 utm-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syCctLq2r4Wk"
      },
      "source": [
        "## Create Project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xjJBSITyXy2",
        "outputId": "1ae9e247-e7c4-4f47-c4cd-df4a6d31013f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!donkey createcar --path /content/mycar"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "________             ______                   _________              \n",
            "___  __ \\_______________  /___________  __    __  ____/_____ ________\n",
            "__  / / /  __ \\_  __ \\_  //_/  _ \\_  / / /    _  /    _  __ `/_  ___/\n",
            "_  /_/ // /_/ /  / / /  ,<  /  __/  /_/ /     / /___  / /_/ /_  /    \n",
            "/_____/ \\____//_/ /_//_/|_| \\___/_\\__, /      \\____/  \\__,_/ /_/     \n",
            "                                 /____/                              \n",
            "\n",
            "using donkey v5.0.dev3 ...\n",
            "Creating car folder: /content/mycar\n",
            "making dir  /content/mycar\n",
            "Creating data & model folders.\n",
            "making dir  /content/mycar/models\n",
            "making dir  /content/mycar/data\n",
            "making dir  /content/mycar/logs\n",
            "Copying car application template: complete\n",
            "Copying car config defaults. Adjust these before starting your car.\n",
            "Copying train script. Adjust these before starting your car.\n",
            "Copying calibrate script. Adjust these before starting your car.\n",
            "Copying my car config overrides\n",
            "Donkey setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W47gmXA0O4Lo"
      },
      "source": [
        "### Download your dataset\n",
        "\n",
        "You can download your dataset to this Colab instance by the following command. You might obtain the uuid of the dataset from the meta.json under each tub."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/mycar/data\n",
        "!curl  -O -J \"https://robocarstore-dc-sg.s3.amazonaws.com/tubs/63783f2f-9497-460b-a6ad-3607c20dcbff/data.tar.gz\"\n"
      ],
      "metadata": {
        "id": "22t7txjxCKld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfSDpXj9x16v"
      },
      "source": [
        "Use the following command to extract the dataset into a folder named tub_xxxxxxx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urhFWBkUyGf0"
      },
      "source": [
        "!tar -xzf data.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, use the \"Getting UUID\" notebook to generate the list of tubs uuid for training"
      ],
      "metadata": {
        "id": "st8m6bQ1JCQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tub_uuids = ['b7da4c7a-d868-46e1-a2e7-1b02c472c8ef', 'a38a0958-baba-4b65-aded-e60e2f22e265']"
      ],
      "metadata": {
        "id": "zoXqqmPiJOTw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tub_uuid in tub_uuids:\n",
        "  !curl -J \"https://robocarstore-dc-sg.s3.amazonaws.com/tubs/{tub_uuid}/data.tar.gz\" -o /tmp/data.tar.gz\n",
        "  !tar -zxf /tmp/data.tar.gz -C /content/mycar/data\n"
      ],
      "metadata": {
        "id": "dyyFNr_KC8xp",
        "outputId": "d72ba1b6-7840-4259-b05f-b358be617076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 21.9M  100 21.9M    0     0  7630k      0  0:00:02  0:00:02 --:--:-- 7628k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 26.3M  100 26.3M    0     0  8258k      0  0:00:03  0:00:03 --:--:-- 8259k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "f4GCu9keDzmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming the latest created folder is the tub_name."
      ],
      "metadata": {
        "id": "4F40oA10DSnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/mycar/data\n",
        "\n",
        "tub_name=!ls -td -- */ | head -n 1\n",
        "tub_name = tub_name[0].strip('/')\n",
        "tub_name"
      ],
      "metadata": {
        "id": "QCNXdA7GCaKg",
        "outputId": "0f4a965a-2286-4ca6-bdd4-59b5605fd67a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mycar/data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tub_16_23-11-30'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyPsgV5KyFkg"
      },
      "source": [
        "Check whether the data is there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsItvBTkzWcH"
      },
      "source": [
        "!ls {tub_name}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check number of images in this tub"
      ],
      "metadata": {
        "id": "gyskUezgrSUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/mycar/data/{tub_name}\n",
        "!find . -type f -name \"*.jpg\" | wc -l"
      ],
      "metadata": {
        "id": "UjFMR-rRrC-m",
        "outputId": "43068a4b-8cc5-4448-dead-2199453f0b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mycar/data/tub_49_24-01-13\n",
            "6623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate a movie for preview"
      ],
      "metadata": {
        "id": "oOnUjZaBELYY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AZvWSeiyqto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f85e0410-c1a1-4dfe-c52e-9c25878a8626"
      },
      "source": [
        "!pip uninstall keras-vis\n",
        "!pip install git+https://github.com/sctse999/keras-vis\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping keras-vis as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/sctse999/keras-vis\n",
            "  Cloning https://github.com/sctse999/keras-vis to /tmp/pip-req-build-1v0t_6g8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/sctse999/keras-vis /tmp/pip-req-build-1v0t_6g8\n",
            "  Resolved https://github.com/sctse999/keras-vis to commit 93e1e803dc0779e25f95b6c1e871d499414e5404\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from keras-vis==0.5.0) (1.16.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from keras-vis==0.5.0) (0.19.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from keras-vis==0.5.0) (3.7.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-vis==0.5.0) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py->keras-vis==0.5.0) (1.25.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras-vis==0.5.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras-vis==0.5.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras-vis==0.5.0) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras-vis==0.5.0) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras-vis==0.5.0) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras-vis==0.5.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras-vis==0.5.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras-vis==0.5.0) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->keras-vis==0.5.0) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->keras-vis==0.5.0) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->keras-vis==0.5.0) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->keras-vis==0.5.0) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->keras-vis==0.5.0) (1.5.0)\n",
            "Building wheels for collected packages: keras-vis\n",
            "  Building wheel for keras-vis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-vis: filename=keras_vis-0.5.0-py2.py3-none-any.whl size=30956 sha256=3c64613095e69adf0c30aa2fda21f1385f4f9a32108eff5de141867d854153b4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z68ih_1r/wheels/76/f4/c7/79f07c4e592d76b0adc5dc32b610988e36a56e8d288d286612\n",
            "Successfully built keras-vis\n",
            "Installing collected packages: keras-vis\n",
            "Successfully installed keras-vis-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/mycar\n",
        "!donkey makemovie --tub ./data/tub* --out {tub_name}.mp4 --start 1 --end 2000"
      ],
      "metadata": {
        "id": "_G0VMYJ_tCCP",
        "outputId": "d4aeae2b-850d-4ee7-8c15-52abc029ccb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mycar\n",
            "________             ______                   _________              \n",
            "___  __ \\_______________  /___________  __    __  ____/_____ ________\n",
            "__  / / /  __ \\_  __ \\_  //_/  _ \\_  / / /    _  /    _  __ `/_  ___/\n",
            "_  /_/ // /_/ /  / / /  ,<  /  __/  /_/ /     / /___  / /_/ /_  /    \n",
            "/_____/ \\____//_/ /_//_/|_| \\___/_\\__, /      \\____/  \\__,_/ /_/     \n",
            "                                 /____/                              \n",
            "\n",
            "using donkey v5.0.dev3 ...\n",
            "usage: makemovie [-h] [--tub TUB] [--out OUT] [--config CONFIG] [--model MODEL] [--type TYPE]\n",
            "                 [--salient] [--start START] [--end END] [--scale SCALE] [--draw-user-input]\n",
            "makemovie: error: unrecognized arguments: ./data/tub_16_23-11-30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download this movie"
      ],
      "metadata": {
        "id": "4d0ZP9JMESFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(f\"{tub_name}.mp4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ca2ON1a5ERxU",
        "outputId": "1b7eec02-fd8f-4376-c89b-47ccf070d2ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_26539a15-b245-43fc-95a0-f04717032a07\", \"tub_49_24-01-13.mp4\", 2657329)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Ya8qEUAfOv"
      },
      "source": [
        "## Train your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edH3xO_AVWXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a4bf22-0fd4-4a43-d8a3-be45251feb2d"
      },
      "source": [
        "%cd /content/mycar\n",
        "\n",
        "model_name = \"model1\"\n",
        "\n",
        "!donkey train --tub ./data/tub* --model models/{model_name}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mycar\n",
            "________             ______                   _________              \n",
            "___  __ \\_______________  /___________  __    __  ____/_____ ________\n",
            "__  / / /  __ \\_  __ \\_  //_/  _ \\_  / / /    _  /    _  __ `/_  ___/\n",
            "_  /_/ // /_/ /  / / /  ,<  /  __/  /_/ /     / /___  / /_/ /_  /    \n",
            "/_____/ \\____//_/ /_//_/|_| \\___/_\\__, /      \\____/  \\__,_/ /_/     \n",
            "                                 /____/                              \n",
            "\n",
            "using donkey v5.0.dev3 ...\n",
            "INFO:donkeycar.config:loading config file: ./config.py\n",
            "INFO:donkeycar.config:loading personal config over-rides from ./myconfig.py\n",
            "2024-02-19 08:19:55.468604: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-19 08:19:55.468665: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-19 08:19:55.469973: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-19 08:19:55.479259: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-19 08:19:56.467375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "WARNING:donkeycar.pipeline.database:No model database found at /content/mycar/models/database.json\n",
            "INFO:donkeycar.utils:get_model_by_type: model type is: linear\n",
            "2024-02-19 08:20:00.346984: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-19 08:20:00.943998: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-19 08:20:00.944319: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-19 08:20:00.945064: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-19 08:20:00.945321: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-19 08:20:00.945516: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-19 08:20:01.161600: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-19 08:20:01.161989: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-19 08:20:01.162126: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2024-02-19 08:20:01.162245: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-02-19 08:20:01.162385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "INFO:donkeycar.parts.keras:Created KerasLinear with interpreter: KerasInterpreter\n",
            "Model: \"linear\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " img_in (InputLayer)         [(None, 120, 160, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 58, 78, 24)           1824      ['img_in[0][0]']              \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 58, 78, 24)           0         ['conv2d_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 27, 37, 32)           19232     ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 27, 37, 32)           0         ['conv2d_2[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 12, 17, 64)           51264     ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 12, 17, 64)           0         ['conv2d_3[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 10, 15, 64)           36928     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 10, 15, 64)           0         ['conv2d_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 8, 13, 64)            36928     ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 8, 13, 64)            0         ['conv2d_5[0][0]']            \n",
            "                                                                                                  \n",
            " flattened (Flatten)         (None, 6656)                 0         ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 100)                  665700    ['flattened[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 100)                  0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 50)                   5050      ['dropout_5[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 50)                   0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " n_outputs0 (Dense)          (None, 1)                    51        ['dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            " n_outputs1 (Dense)          (None, 1)                    51        ['dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 817028 (3.12 MB)\n",
            "Trainable params: 817028 (3.12 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "INFO:donkeycar.parts.datastore_v2:Found datastore at /content/mycar/data/tub_15_23-11-30\n",
            "INFO:donkeycar.parts.datastore_v2:Using last catalog /content/mycar/data/tub_15_23-11-30/catalog_4.catalog\n",
            "INFO:donkeycar.parts.datastore_v2:Found datastore at /content/mycar/data/tub_16_23-11-30\n",
            "INFO:donkeycar.parts.datastore_v2:Using last catalog /content/mycar/data/tub_16_23-11-30/catalog_5.catalog\n",
            "INFO:donkeycar.pipeline.types:Loading tubs from paths ['./data/tub_15_23-11-30', './data/tub_16_23-11-30']\n",
            "INFO:donkeycar.pipeline.training:Records # Training 8092\n",
            "INFO:donkeycar.pipeline.training:Records # Validation 2024\n",
            "INFO:donkeycar.parts.tub_v2:Closing tub ./data/tub_15_23-11-30\n",
            "INFO:donkeycar.parts.tub_v2:Closing tub ./data/tub_16_23-11-30\n",
            "INFO:donkeycar.parts.image_transformations:Creating ImageTransformations []\n",
            "INFO:donkeycar.parts.image_transformations:Creating ImageTransformations []\n",
            "INFO:donkeycar.parts.image_transformations:Creating ImageTransformations []\n",
            "INFO:donkeycar.parts.image_transformations:Creating ImageTransformations []\n",
            "INFO:donkeycar.pipeline.training:Train with image caching: True\n",
            "INFO:donkeycar.parts.keras:////////// Starting training //////////\n",
            "Epoch 1/100\n",
            "2024-02-19 08:20:02.873226: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inlinear/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
            "2024-02-19 08:20:03.445587: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
            "2024-02-19 08:20:06.859349: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd8d4bf3f00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-02-19 08:20:06.859395: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2024-02-19 08:20:06.882123: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1708330807.054422    2139 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.7785 - n_outputs0_loss: 0.5409 - n_outputs1_loss: 0.2376\n",
            "Epoch 1: val_loss improved from inf to 0.65133, saving model to /content/mycar/models/model1\n",
            "INFO:tensorflow:Assets written to: /content/mycar/models/model1/assets\n",
            "64/64 [==============================] - 26s 283ms/step - loss: 0.7785 - n_outputs0_loss: 0.5409 - n_outputs1_loss: 0.2376 - val_loss: 0.6513 - val_n_outputs0_loss: 0.4760 - val_n_outputs1_loss: 0.1753\n",
            "Epoch 2/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.6043 - n_outputs0_loss: 0.4147 - n_outputs1_loss: 0.1896\n",
            "Epoch 2: val_loss improved from 0.65133 to 0.51543, saving model to /content/mycar/models/model1\n",
            "INFO:tensorflow:Assets written to: /content/mycar/models/model1/assets\n",
            "64/64 [==============================] - 10s 158ms/step - loss: 0.6043 - n_outputs0_loss: 0.4147 - n_outputs1_loss: 0.1896 - val_loss: 0.5154 - val_n_outputs0_loss: 0.3596 - val_n_outputs1_loss: 0.1558\n",
            "Epoch 3/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.5375 - n_outputs0_loss: 0.3649 - n_outputs1_loss: 0.1726\n",
            "Epoch 3: val_loss improved from 0.51543 to 0.44409, saving model to /content/mycar/models/model1\n",
            "INFO:tensorflow:Assets written to: /content/mycar/models/model1/assets\n",
            "64/64 [==============================] - 10s 160ms/step - loss: 0.5375 - n_outputs0_loss: 0.3649 - n_outputs1_loss: 0.1726 - val_loss: 0.4441 - val_n_outputs0_loss: 0.3069 - val_n_outputs1_loss: 0.1372\n",
            "Epoch 4/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.4507 - n_outputs0_loss: 0.2883 - n_outputs1_loss: 0.1623\n",
            "Epoch 4: val_loss improved from 0.44409 to 0.36151, saving model to /content/mycar/models/model1\n",
            "INFO:tensorflow:Assets written to: /content/mycar/models/model1/assets\n",
            "64/64 [==============================] - 10s 162ms/step - loss: 0.4507 - n_outputs0_loss: 0.2883 - n_outputs1_loss: 0.1623 - val_loss: 0.3615 - val_n_outputs0_loss: 0.2295 - val_n_outputs1_loss: 0.1320\n",
            "Epoch 5/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.3928 - n_outputs0_loss: 0.2416 - n_outputs1_loss: 0.1512\n",
            "Epoch 5: val_loss improved from 0.36151 to 0.31245, saving model to /content/mycar/models/model1\n",
            "INFO:tensorflow:Assets written to: /content/mycar/models/model1/assets\n",
            "64/64 [==============================] - 12s 194ms/step - loss: 0.3928 - n_outputs0_loss: 0.2416 - n_outputs1_loss: 0.1512 - val_loss: 0.3125 - val_n_outputs0_loss: 0.1943 - val_n_outputs1_loss: 0.1182\n",
            "Epoch 6/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.3581 - n_outputs0_loss: 0.2191 - n_outputs1_loss: 0.1390"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quiz\n",
        "1. How many records are used for training?\n",
        "2. How many records are used for validation?\n",
        "3. Does the sum of this number adds up to the number of images in the dataset?\n",
        "4. What is the final lost value?\n",
        "5. How long does this training last?\n",
        "6. How many epoches in this training?\n",
        "7. How many steps in this epoch?\n",
        "8. What's the value of `MAX_EPOCHS` in `myconfig.py`?\n",
        "9. Why the training stop before it reaches the 100th epoch?"
      ],
      "metadata": {
        "id": "HeHjt60xKphz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXzn1noJz5MQ"
      },
      "source": [
        "Check if the model is generated\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b9vJV4EzlO8"
      },
      "source": [
        "!ls -alh /content/mycar/models/{model_name}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AixQrFy_z3vv"
      },
      "source": [
        "%cd /content/mycar/models/{model_name}\n",
        "\n",
        "import glob\n",
        "file = glob.glob(\"*.png\")\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(file[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BEOJYH601O0"
      },
      "source": [
        "## Copy the trained model back to Donkey Car (Pi)\n",
        "\n",
        "Once the training is complete on colab, download the model file under /content/mycar/models/ folder location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtvyJpOdocjb"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('./mypilot.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7AIY6yBOCM-"
      },
      "source": [
        "Alternatively, you can copy the model back to Google Drive too"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dim4fCpOBo9"
      },
      "source": [
        "!cp /content/mycar/models/mypilot.h5 /content/drive/My\\ Drive/mycar/models/mypilot.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpkOVzh86omO"
      },
      "source": [
        "### Copy the file from your PC or Mac to the Raspberry Pi using Filezilla or scp command.\n",
        "\n",
        "```\n",
        "sftp pi@raspberry.local\n",
        "cd mycar/models\n",
        "put mypilot.h5\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfERkGy821Xy"
      },
      "source": [
        "## Start Autopilot on Pi\n",
        "\n",
        "\n",
        "```bash\n",
        "cd ~/mycar\n",
        "python manage.py drive --model models/mypilot.h5 --js\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changing training parameters"
      ],
      "metadata": {
        "id": "7vBkLcqCKMHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change number of epoch"
      ],
      "metadata": {
        "id": "7itOFJ_8KP8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Edit `myconfig.py`\n",
        "2. Change `MAX_EPOCHS` to 5, 10 , 20\n",
        "3. Training the model again by executing the training script above\n",
        "4. What is `MAX_EPOCHS` used for?\n",
        "5. Why do we want to change `MAX_EPOCHS`?"
      ],
      "metadata": {
        "id": "ZgnRGiMtLSHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change TRAIN_FILTER"
      ],
      "metadata": {
        "id": "jy5ax0YlKTyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change TRAIN_FILTER in `myconfig.py` to > 0.1 and check the number of images used for training and validation"
      ],
      "metadata": {
        "id": "CCnJoXqLKeDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change Batch Size"
      ],
      "metadata": {
        "id": "J7Nn7rjkLVRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Edit `myconfig.py`\n",
        "2. Change `BATCH_SIZE` to 256\n",
        "3. Training the model again by executing the training script above\n",
        "4. Does the training go faster? Why?\n"
      ],
      "metadata": {
        "id": "Zv_4oluqLXa4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X93SodzAv9hV"
      },
      "source": [
        "## Bonus - Salient Object Visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imageio-ffmpeg\n"
      ],
      "metadata": {
        "id": "1nGatrwZiTEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKI37gVrv9Q8"
      },
      "source": [
        "%cd /content/mycar\n",
        "!donkey makemovie --tub data/{tub_name} --model models/mypilot.h5 --type linear --salient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcUrgOq_pePV"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "%cd /content/mycar\n",
        "!ls -ahl\n",
        "files.download('tub_movie.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}